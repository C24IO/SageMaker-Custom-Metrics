#!/usr/bin/env python3

# This file implements the scoring service shell. You don't necessarily need to modify it for various
# algorithms. It starts nginx and gunicorn with the correct configurations and then simply waits until
# gunicorn exits.
#
# The flask server is specified to be the app object in wsgi.py
#
# We set the following parameters:
#
# Parameter                Environment Variable              Default Value
# ---------                --------------------              -------------
# number of workers        MODEL_SERVER_WORKERS              the number of CPU cores
# timeout                  MODEL_SERVER_TIMEOUT              60 seconds

from __future__ import print_function

import multiprocessing
import os
import signal
import subprocess
import sys

import logging
import threading
import time
import urllib.request
import boto3
import json
import os
from datetime import datetime
from time import sleep
import subprocess

from random import seed
from random import randint

# seed random number generator
seed(datetime.now())

### CHOOSE REGION ####
EC2_REGION = 'us-east-1'

### CHOOSE NAMESPACE PARMETERS HERE###
my_NameSpace = 'CustomMetric-G1' 

### CHOOSE PUSH INTERVAL ####
sleep_interval = 10

### CHOOSE STORAGE RESOLUTION (BETWEEN 1-60) ####
store_reso = 1

#Instance information
BASE_URL = 'http://169.254.169.254/latest/meta-data/'
INSTANCE_ID = 'i-061e3a5017ab48e05' #urllib.request.urlopen(BASE_URL + 'instance-id').read().decode("utf-8")
IMAGE_ID = 'ami-0cfa188dfa2154f54' #urllib.request.urlopen(BASE_URL + 'ami-id').read().decode("utf-8")
INSTANCE_TYPE = 'p3.2xlarge' #urllib.request.urlopen(BASE_URL + 'instance-type').read().decode("utf-8")
INSTANCE_AZ = 'us-east-2b' #urllib.request.urlopen(BASE_URL + 'placement/availability-zone').read()
EC2_REGION = 'us-east-1' #INSTANCE_AZ[:-1]

TIMESTAMP = datetime.now().strftime('%Y-%m-%dT%H')
TMP_FILE = '/tmp/GPU_TEMP'
TMP_FILE_SAVED = TMP_FILE + TIMESTAMP

# Create CloudWatch client
#cloudwatch = boto3.client('cloudwatch', region_name=EC2_REGION.decode('utf-8'))
cloudwatch = boto3.client('cloudwatch', region_name=EC2_REGION)
    
# Flag to push to CloudWatch
PUSH_TO_CW = True

class Util:
    def __init__(self, gpu, memory):
        self.gpu = gpu
        self.memory = memory

def getPowerDraw(handle):
    powDrawStr = str(randint(9, 90))
    return powDrawStr

def getTemp(handle):
    temp = str(randint(9, 90))
    return temp

def getUtilization(handle):    
    util = Util(randint(9, 90), randint(9, 90))
    gpu_util = str(util.gpu)
    mem_util = str(util.memory)
    return util, gpu_util, mem_util

def logResults(i, util, gpu_util, mem_util, powDrawStr, temp):
    try:
        gpu_logs = open(TMP_FILE_SAVED, 'a+')
        writeString = str(i) + ',' + gpu_util + ',' + mem_util + ',' + powDrawStr + ',' + temp + '\n'
        gpu_logs.write(writeString)
    except:
        print("Error writing to file ", gpu_logs)
    finally:
        gpu_logs.close()
    if (PUSH_TO_CW):
        MY_DIMENSIONS=[
                    {
                        'Name': 'InstanceId',
                        'Value': str(INSTANCE_ID)
                    },
                    {
                        'Name': 'ImageId',
                        'Value': str(IMAGE_ID)
                    },
                    {
                        'Name': 'InstanceType',
                        'Value': str(INSTANCE_TYPE)
                    },
                    {
                        'Name': 'GPUNumber',
                        'Value': str(i)
                    }
                ]
        cloudwatch.put_metric_data(
            MetricData=[
                {
                    'MetricName': 'GPU Usage',
                    'Dimensions': MY_DIMENSIONS,
                    'Unit': 'Percent',
                    'StorageResolution': store_reso,
                    'Value': util.gpu
                },
                {
                    'MetricName': 'Memory Usage',
                    'Dimensions': MY_DIMENSIONS,
                    'Unit': 'Percent',
                    'StorageResolution': store_reso,
                    'Value': util.memory
                },
                {
                    'MetricName': 'Power Usage (Watts)',
                    'Dimensions': MY_DIMENSIONS,
                    'Unit': 'None',
                    'StorageResolution': store_reso,
                    'Value': float(powDrawStr)
                },
                {
                    'MetricName': 'Temperature (C)',
                    'Dimensions': MY_DIMENSIONS,
                    'Unit': 'None',
                    'StorageResolution': store_reso,
                    'Value': int(temp)
                },            
        ],
            Namespace=my_NameSpace
        )

def log_custom_GPU_metrics():
    deviceCount = 1
    while True:
        time.sleep(.166)
        for gpu in range(deviceCount):
            handle = 'dummy'
            util, gpu_util, mem_util =  getUtilization(handle)
            logResults(gpu, util, gpu_util, mem_util, getPowerDraw(handle), getTemp(handle))       
            
cw_logger_thread = threading.Thread(target=log_custom_GPU_metrics)
cw_logger_thread.setDaemon(True)
cw_logger_thread.start()
print('Started')
#cw_logger_thread.join()
print('Launched')            

cpu_count = multiprocessing.cpu_count()

model_server_timeout = os.environ.get('MODEL_SERVER_TIMEOUT', 60)
model_server_workers = int(os.environ.get('MODEL_SERVER_WORKERS', cpu_count))
model_threads = int(1)
#model_server_workers = int(4)

def sigterm_handler(nginx_pid, gunicorn_pid):
    try:
        os.kill(nginx_pid, signal.SIGQUIT)
    except OSError:
        pass
    try:
        os.kill(gunicorn_pid, signal.SIGTERM)
    except OSError:
        pass

    sys.exit(0)

def start_server():
    print('Starting the inference server with {} workers.'.format(model_server_workers))


    # link the log streams to stdout/err so they will be logged to the container logs
    subprocess.check_call(['ln', '-sf', '/dev/stdout', '/var/log/nginx/access.log'])
    subprocess.check_call(['ln', '-sf', '/dev/stderr', '/var/log/nginx/error.log'])

    nginx = subprocess.Popen(['nginx', '-c', '/opt/program/nginx.conf'])
    gunicorn = subprocess.Popen(['gunicorn',
                                 '--timeout', str(model_server_timeout),
                                 '-k', 'gevent',
                                 '--threads', str(model_threads),
                                 '-b', 'unix:/tmp/gunicorn.sock',
                                 '-w', str(model_server_workers),
                                 'wsgi:app'])

    signal.signal(signal.SIGTERM, lambda a, b: sigterm_handler(nginx.pid, gunicorn.pid))

    # If either subprocess exits, so do we.
    pids = set([nginx.pid, gunicorn.pid])
    while True:
        pid, _ = os.wait()
        if pid in pids:
            break

    sigterm_handler(nginx.pid, gunicorn.pid)
    print('Inference server exiting')

# The main routine just invokes the start function.

if __name__ == '__main__':
    start_server()
