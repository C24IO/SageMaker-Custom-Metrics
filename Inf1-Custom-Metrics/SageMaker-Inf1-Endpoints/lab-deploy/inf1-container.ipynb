{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[37m# This is the file that implements a flask server to do inferences. It's the file that you will modify to\u001b[39;49;00m\n",
      "\u001b[37m# implement the scoring for your own algorithm.\u001b[39;49;00m\n",
      "\n",
      "\u001b[34mfrom\u001b[39;49;00m \u001b[04m\u001b[36m__future__\u001b[39;49;00m \u001b[34mimport\u001b[39;49;00m print_function\n",
      "\n",
      "\u001b[34mimport\u001b[39;49;00m \u001b[04m\u001b[36mos\u001b[39;49;00m\n",
      "\u001b[34mimport\u001b[39;49;00m \u001b[04m\u001b[36mjson\u001b[39;49;00m\n",
      "\u001b[34mimport\u001b[39;49;00m \u001b[04m\u001b[36mpickle\u001b[39;49;00m\n",
      "\u001b[34mimport\u001b[39;49;00m \u001b[04m\u001b[36msys\u001b[39;49;00m\n",
      "\u001b[34mimport\u001b[39;49;00m \u001b[04m\u001b[36msignal\u001b[39;49;00m\n",
      "\u001b[34mimport\u001b[39;49;00m \u001b[04m\u001b[36mtraceback\u001b[39;49;00m\n",
      "\u001b[34mimport\u001b[39;49;00m \u001b[04m\u001b[36mpickle\u001b[39;49;00m\n",
      "\u001b[34mimport\u001b[39;49;00m \u001b[04m\u001b[36mtorch\u001b[39;49;00m\n",
      "\u001b[34mimport\u001b[39;49;00m \u001b[04m\u001b[36mtorch_neuron\u001b[39;49;00m\n",
      "\u001b[34mimport\u001b[39;49;00m \u001b[04m\u001b[36mtransformers\u001b[39;49;00m\n",
      "\u001b[34mfrom\u001b[39;49;00m \u001b[04m\u001b[36mtransformers\u001b[39;49;00m \u001b[34mimport\u001b[39;49;00m BertTokenizer\n",
      "\u001b[34mfrom\u001b[39;49;00m \u001b[04m\u001b[36mtransformers\u001b[39;49;00m \u001b[34mimport\u001b[39;49;00m BertModel\n",
      "\u001b[34mimport\u001b[39;49;00m \u001b[04m\u001b[36mmath\u001b[39;49;00m\n",
      "\u001b[34mfrom\u001b[39;49;00m \u001b[04m\u001b[36mtransformers\u001b[39;49;00m \u001b[34mimport\u001b[39;49;00m AutoTokenizer, AutoModelForSequenceClassification\n",
      "\n",
      "\u001b[34mimport\u001b[39;49;00m \u001b[04m\u001b[36mlogging\u001b[39;49;00m\n",
      "\u001b[34mimport\u001b[39;49;00m \u001b[04m\u001b[36mnumpy\u001b[39;49;00m \u001b[34mas\u001b[39;49;00m \u001b[04m\u001b[36mnp\u001b[39;49;00m\n",
      "\u001b[34mimport\u001b[39;49;00m \u001b[04m\u001b[36mio\u001b[39;49;00m\n",
      "\u001b[34mimport\u001b[39;49;00m \u001b[04m\u001b[36murllib\u001b[39;49;00m\u001b[04m\u001b[36m.\u001b[39;49;00m\u001b[04m\u001b[36mrequest\u001b[39;49;00m\n",
      "\n",
      "\n",
      "\n",
      "\u001b[34mtry\u001b[39;49;00m:\n",
      "    \u001b[34mfrom\u001b[39;49;00m \u001b[04m\u001b[36mStringIO\u001b[39;49;00m \u001b[34mimport\u001b[39;49;00m StringIO \u001b[37m## for Python 2\u001b[39;49;00m\n",
      "\u001b[34mexcept\u001b[39;49;00m \u001b[36mImportError\u001b[39;49;00m:\n",
      "    \u001b[34mfrom\u001b[39;49;00m \u001b[04m\u001b[36mio\u001b[39;49;00m \u001b[34mimport\u001b[39;49;00m StringIO \u001b[37m## for Python 3\u001b[39;49;00m\n",
      "\n",
      "\u001b[34mimport\u001b[39;49;00m \u001b[04m\u001b[36mflask\u001b[39;49;00m\n",
      "\n",
      "\u001b[34mimport\u001b[39;49;00m \u001b[04m\u001b[36mpandas\u001b[39;49;00m \u001b[34mas\u001b[39;49;00m \u001b[04m\u001b[36mpd\u001b[39;49;00m\n",
      "\n",
      "prefix = \u001b[33m'\u001b[39;49;00m\u001b[33m/opt/ml/\u001b[39;49;00m\u001b[33m'\u001b[39;49;00m\n",
      "model_path = os.path.join(prefix, \u001b[33m'\u001b[39;49;00m\u001b[33mmodel\u001b[39;49;00m\u001b[33m'\u001b[39;49;00m)\n",
      "\n",
      "\u001b[37m# A singleton for holding the model. This simply loads the model and holds it.\u001b[39;49;00m\n",
      "\u001b[37m# It has a predict function that does a prediction based on the model and the input data.\u001b[39;49;00m\n",
      "\n",
      "\u001b[34mclass\u001b[39;49;00m \u001b[04m\u001b[32mScoringService\u001b[39;49;00m(\u001b[36mobject\u001b[39;49;00m):\n",
      "    model = \u001b[34mNone\u001b[39;49;00m                \u001b[37m# Where we keep the model when it's loaded\u001b[39;49;00m\n",
      "\n",
      "    \u001b[90m@classmethod\u001b[39;49;00m\n",
      "    \u001b[34mdef\u001b[39;49;00m \u001b[32mget_model\u001b[39;49;00m(\u001b[36mcls\u001b[39;49;00m):\n",
      "        \u001b[33m\"\"\"Get the model object for this instance, loading it if it's not already loaded.\"\"\"\u001b[39;49;00m        \n",
      "\n",
      "        \u001b[34mif\u001b[39;49;00m \u001b[36mcls\u001b[39;49;00m.model == \u001b[34mNone\u001b[39;49;00m:\n",
      "            \u001b[36mcls\u001b[39;49;00m.model = torch.jit.load(os.path.join(model_path, \u001b[33m'\u001b[39;49;00m\u001b[33mneuron_compiled_model.pt\u001b[39;49;00m\u001b[33m'\u001b[39;49;00m))\n",
      "            \u001b[36mprint\u001b[39;49;00m(\u001b[33m'\u001b[39;49;00m\u001b[33mmodel loaded\u001b[39;49;00m\u001b[33m'\u001b[39;49;00m)\n",
      "            \u001b[36mprint\u001b[39;49;00m(\u001b[36mcls\u001b[39;49;00m.model)\n",
      "            \n",
      "        \u001b[34mreturn\u001b[39;49;00m \u001b[36mcls\u001b[39;49;00m.model\n",
      "\n",
      "    \u001b[90m@classmethod\u001b[39;49;00m\n",
      "    \u001b[34mdef\u001b[39;49;00m \u001b[32mpredict\u001b[39;49;00m(\u001b[36mcls\u001b[39;49;00m, *\u001b[36minput\u001b[39;49;00m):\n",
      "        \n",
      "        \u001b[36mprint\u001b[39;49;00m(\u001b[33m'\u001b[39;49;00m\u001b[33mpredict\u001b[39;49;00m\u001b[33m'\u001b[39;49;00m)\n",
      "        \u001b[33m\"\"\"For the input, do the predictions and return them.\u001b[39;49;00m\n",
      "\u001b[33m\u001b[39;49;00m\n",
      "\u001b[33m        Args:\u001b[39;49;00m\n",
      "\u001b[33m            input (a pandas dataframe): The data on which to do the predictions. There will be\u001b[39;49;00m\n",
      "\u001b[33m                one prediction per row in the dataframe\"\"\"\u001b[39;49;00m\n",
      "        clf = \u001b[36mcls\u001b[39;49;00m.get_model()\n",
      "        \n",
      "        \u001b[37m#print(type(*input))\u001b[39;49;00m\n",
      "        \u001b[36mprint\u001b[39;49;00m(*\u001b[36minput\u001b[39;49;00m)\n",
      "        \u001b[34mreturn\u001b[39;49;00m clf(*\u001b[36minput\u001b[39;49;00m)\n",
      "\n",
      "\u001b[37m# The flask app for serving predictions\u001b[39;49;00m\n",
      "app = flask.Flask(\u001b[31m__name__\u001b[39;49;00m)\n",
      "\n",
      "\u001b[90m@app\u001b[39;49;00m.route(\u001b[33m'\u001b[39;49;00m\u001b[33m/ping\u001b[39;49;00m\u001b[33m'\u001b[39;49;00m, methods=[\u001b[33m'\u001b[39;49;00m\u001b[33mGET\u001b[39;49;00m\u001b[33m'\u001b[39;49;00m])\n",
      "\u001b[34mdef\u001b[39;49;00m \u001b[32mping\u001b[39;49;00m():\n",
      "    \u001b[33m\"\"\"Determine if the container is working and healthy. In this sample container, we declare\u001b[39;49;00m\n",
      "\u001b[33m    it healthy if we can load the model successfully.\"\"\"\u001b[39;49;00m\n",
      "    health = ScoringService.get_model() \u001b[35mis\u001b[39;49;00m \u001b[35mnot\u001b[39;49;00m \u001b[34mNone\u001b[39;49;00m  \u001b[37m# You can insert a health check here\u001b[39;49;00m\n",
      "    \u001b[36mprint\u001b[39;49;00m(health)\n",
      "    \u001b[37m#health = True \u001b[39;49;00m\n",
      "\n",
      "    status = \u001b[34m200\u001b[39;49;00m \u001b[34mif\u001b[39;49;00m health \u001b[34melse\u001b[39;49;00m \u001b[34m404\u001b[39;49;00m\n",
      "    \n",
      "    \u001b[34mreturn\u001b[39;49;00m flask.Response(response=\u001b[33m'\u001b[39;49;00m\u001b[33m\\n\u001b[39;49;00m\u001b[33m'\u001b[39;49;00m, status=status, mimetype=\u001b[33m'\u001b[39;49;00m\u001b[33mapplication/json\u001b[39;49;00m\u001b[33m'\u001b[39;49;00m)\n",
      "\n",
      "\u001b[90m@app\u001b[39;49;00m.route(\u001b[33m'\u001b[39;49;00m\u001b[33m/invocations\u001b[39;49;00m\u001b[33m'\u001b[39;49;00m, methods=[\u001b[33m'\u001b[39;49;00m\u001b[33mPOST\u001b[39;49;00m\u001b[33m'\u001b[39;49;00m])\n",
      "\u001b[34mdef\u001b[39;49;00m \u001b[32mtransformation\u001b[39;49;00m():\n",
      "    \u001b[33m\"\"\"Do an inference on a single batch of data. In this sample server, we take data as CSV, convert\u001b[39;49;00m\n",
      "\u001b[33m    it to a pandas data frame for internal use and then convert the predictions back to CSV (which really\u001b[39;49;00m\n",
      "\u001b[33m    just means one prediction per line, since there's a single column.\u001b[39;49;00m\n",
      "\u001b[33m    \"\"\"\u001b[39;49;00m\n",
      "    data = \u001b[34mNone\u001b[39;49;00m\n",
      "    \n",
      "    \u001b[36mprint\u001b[39;49;00m(flask.request.content_type)\n",
      "        \n",
      "    pickled_bytes = flask.request.data\n",
      "    encoded_sentence_tuple = pickle.loads(pickled_bytes)\n",
      "    \u001b[36mprint\u001b[39;49;00m(\u001b[36mtype\u001b[39;49;00m(encoded_sentence_tuple))\n",
      "    \u001b[37m#input_statement = encoded_sentence['input_ids'], encoded_sentence['attention_mask'], encoded_sentence['token_type_ids']  \u001b[39;49;00m\n",
      "    \u001b[36mprint\u001b[39;49;00m(encoded_sentence_tuple)\n",
      "    embedding = ScoringService.predict(*encoded_sentence_tuple)\n",
      "    \u001b[36mprint\u001b[39;49;00m(\u001b[36mtype\u001b[39;49;00m(embedding))\n",
      "    \u001b[36mprint\u001b[39;49;00m(embedding)\n",
      "    raw_bytes_embedding = pickle.dumps(embedding)\n",
      "    \u001b[36mprint\u001b[39;49;00m(\u001b[33m'\u001b[39;49;00m\u001b[33mScored\u001b[39;49;00m\u001b[33m'\u001b[39;49;00m)\n",
      "         \n",
      "    result = raw_bytes_embedding\n",
      "    \u001b[34mreturn\u001b[39;49;00m flask.Response(response=result, status=\u001b[34m200\u001b[39;49;00m, mimetype=\u001b[33m'\u001b[39;49;00m\u001b[33mapplication/binary\u001b[39;49;00m\u001b[33m'\u001b[39;49;00m)\n"
     ]
    }
   ],
   "source": [
    "!pygmentize code/predictor.py"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "#!/usr/bin/env python3\n",
      "\n",
      "# This file implements the scoring service shell. You don't necessarily need to modify it for various\n",
      "# algorithms. It starts nginx and gunicorn with the correct configurations and then simply waits until\n",
      "# gunicorn exits.\n",
      "#\n",
      "# The flask server is specified to be the app object in wsgi.py\n",
      "#\n",
      "# We set the following parameters:\n",
      "#\n",
      "# Parameter                Environment Variable              Default Value\n",
      "# ---------                --------------------              -------------\n",
      "# number of workers        MODEL_SERVER_WORKERS              the number of CPU cores\n",
      "# timeout                  MODEL_SERVER_TIMEOUT              60 seconds\n",
      "\n",
      "from __future__ import print_function\n",
      "\n",
      "import multiprocessing\n",
      "import os\n",
      "import signal\n",
      "import subprocess\n",
      "import sys\n",
      "\n",
      "import logging\n",
      "import threading\n",
      "import time\n",
      "import urllib.request\n",
      "import boto3\n",
      "import json\n",
      "import os\n",
      "from datetime import datetime\n",
      "from time import sleep\n",
      "import subprocess\n",
      "\n",
      "from random import seed\n",
      "from random import randint\n",
      "\n",
      "# seed random number generator\n",
      "seed(datetime.now())\n",
      "\n",
      "### CHOOSE REGION ####\n",
      "EC2_REGION = 'us-east-1'\n",
      "\n",
      "### CHOOSE NAMESPACE PARMETERS HERE###\n",
      "my_NameSpace = 'CustomMetric-G1' \n",
      "\n",
      "### CHOOSE PUSH INTERVAL ####\n",
      "sleep_interval = 10\n",
      "\n",
      "### CHOOSE STORAGE RESOLUTION (BETWEEN 1-60) ####\n",
      "store_reso = 1\n",
      "\n",
      "#Instance information\n",
      "BASE_URL = 'http://169.254.169.254/latest/meta-data/'\n",
      "INSTANCE_ID = 'i-061e3a5017ab48e05' #urllib.request.urlopen(BASE_URL + 'instance-id').read().decode(\"utf-8\")\n",
      "IMAGE_ID = 'ami-0cfa188dfa2154f54' #urllib.request.urlopen(BASE_URL + 'ami-id').read().decode(\"utf-8\")\n",
      "INSTANCE_TYPE = 'p3.2xlarge' #urllib.request.urlopen(BASE_URL + 'instance-type').read().decode(\"utf-8\")\n",
      "INSTANCE_AZ = 'us-east-2b' #urllib.request.urlopen(BASE_URL + 'placement/availability-zone').read()\n",
      "EC2_REGION = 'us-east-1' #INSTANCE_AZ[:-1]\n",
      "\n",
      "TIMESTAMP = datetime.now().strftime('%Y-%m-%dT%H')\n",
      "TMP_FILE = '/tmp/GPU_TEMP'\n",
      "TMP_FILE_SAVED = TMP_FILE + TIMESTAMP\n",
      "\n",
      "# Create CloudWatch client\n",
      "#cloudwatch = boto3.client('cloudwatch', region_name=EC2_REGION.decode('utf-8'))\n",
      "cloudwatch = boto3.client('cloudwatch', region_name=EC2_REGION)\n",
      "    \n",
      "# Flag to push to CloudWatch\n",
      "PUSH_TO_CW = True\n",
      "\n",
      "class Util:\n",
      "    def __init__(self, gpu, memory):\n",
      "        self.gpu = gpu\n",
      "        self.memory = memory\n",
      "\n",
      "def getPowerDraw(handle):\n",
      "    powDrawStr = str(randint(9, 90))\n",
      "    return powDrawStr\n",
      "\n",
      "def getTemp(handle):\n",
      "    temp = str(randint(9, 90))\n",
      "    return temp\n",
      "\n",
      "def getUtilization(handle):    \n",
      "    util = Util(randint(9, 90), randint(9, 90))\n",
      "    gpu_util = str(util.gpu)\n",
      "    mem_util = str(util.memory)\n",
      "    return util, gpu_util, mem_util\n",
      "\n",
      "def logResults(i, util, gpu_util, mem_util, powDrawStr, temp):\n",
      "    try:\n",
      "        gpu_logs = open(TMP_FILE_SAVED, 'a+')\n",
      "        writeString = str(i) + ',' + gpu_util + ',' + mem_util + ',' + powDrawStr + ',' + temp + '\\n'\n",
      "        gpu_logs.write(writeString)\n",
      "    except:\n",
      "        print(\"Error writing to file \", gpu_logs)\n",
      "    finally:\n",
      "        gpu_logs.close()\n",
      "    if (PUSH_TO_CW):\n",
      "        MY_DIMENSIONS=[\n",
      "                    {\n",
      "                        'Name': 'InstanceId',\n",
      "                        'Value': str(INSTANCE_ID)\n",
      "                    },\n",
      "                    {\n",
      "                        'Name': 'ImageId',\n",
      "                        'Value': str(IMAGE_ID)\n",
      "                    },\n",
      "                    {\n",
      "                        'Name': 'InstanceType',\n",
      "                        'Value': str(INSTANCE_TYPE)\n",
      "                    },\n",
      "                    {\n",
      "                        'Name': 'GPUNumber',\n",
      "                        'Value': str(i)\n",
      "                    }\n",
      "                ]\n",
      "        cloudwatch.put_metric_data(\n",
      "            MetricData=[\n",
      "                {\n",
      "                    'MetricName': 'GPU Usage',\n",
      "                    'Dimensions': MY_DIMENSIONS,\n",
      "                    'Unit': 'Percent',\n",
      "                    'StorageResolution': store_reso,\n",
      "                    'Value': util.gpu\n",
      "                },\n",
      "                {\n",
      "                    'MetricName': 'Memory Usage',\n",
      "                    'Dimensions': MY_DIMENSIONS,\n",
      "                    'Unit': 'Percent',\n",
      "                    'StorageResolution': store_reso,\n",
      "                    'Value': util.memory\n",
      "                },\n",
      "                {\n",
      "                    'MetricName': 'Power Usage (Watts)',\n",
      "                    'Dimensions': MY_DIMENSIONS,\n",
      "                    'Unit': 'None',\n",
      "                    'StorageResolution': store_reso,\n",
      "                    'Value': float(powDrawStr)\n",
      "                },\n",
      "                {\n",
      "                    'MetricName': 'Temperature (C)',\n",
      "                    'Dimensions': MY_DIMENSIONS,\n",
      "                    'Unit': 'None',\n",
      "                    'StorageResolution': store_reso,\n",
      "                    'Value': int(temp)\n",
      "                },            \n",
      "        ],\n",
      "            Namespace=my_NameSpace\n",
      "        )\n",
      "\n",
      "def log_custom_GPU_metrics():\n",
      "    deviceCount = 1\n",
      "    while True:\n",
      "        time.sleep(.166)\n",
      "        for gpu in range(deviceCount):\n",
      "            handle = 'dummy'\n",
      "            util, gpu_util, mem_util =  getUtilization(handle)\n",
      "            logResults(gpu, util, gpu_util, mem_util, getPowerDraw(handle), getTemp(handle))       \n",
      "            \n",
      "cw_logger_thread = threading.Thread(target=log_custom_GPU_metrics)\n",
      "cw_logger_thread.setDaemon(True)\n",
      "cw_logger_thread.start()\n",
      "print('Started')\n",
      "#cw_logger_thread.join()\n",
      "print('Launched')            \n",
      "\n",
      "cpu_count = multiprocessing.cpu_count()\n",
      "\n",
      "model_server_timeout = os.environ.get('MODEL_SERVER_TIMEOUT', 60)\n",
      "model_server_workers = int(os.environ.get('MODEL_SERVER_WORKERS', cpu_count))\n",
      "model_threads = int(1)\n",
      "#model_server_workers = int(4)\n",
      "\n",
      "def sigterm_handler(nginx_pid, gunicorn_pid):\n",
      "    try:\n",
      "        os.kill(nginx_pid, signal.SIGQUIT)\n",
      "    except OSError:\n",
      "        pass\n",
      "    try:\n",
      "        os.kill(gunicorn_pid, signal.SIGTERM)\n",
      "    except OSError:\n",
      "        pass\n",
      "\n",
      "    sys.exit(0)\n",
      "\n",
      "def start_server():\n",
      "    print('Starting the inference server with {} workers.'.format(model_server_workers))\n",
      "\n",
      "\n",
      "    # link the log streams to stdout/err so they will be logged to the container logs\n",
      "    subprocess.check_call(['ln', '-sf', '/dev/stdout', '/var/log/nginx/access.log'])\n",
      "    subprocess.check_call(['ln', '-sf', '/dev/stderr', '/var/log/nginx/error.log'])\n",
      "\n",
      "    nginx = subprocess.Popen(['nginx', '-c', '/opt/program/nginx.conf'])\n",
      "    gunicorn = subprocess.Popen(['gunicorn',\n",
      "                                 '--timeout', str(model_server_timeout),\n",
      "                                 '-k', 'gevent',\n",
      "                                 '--threads', str(model_threads),\n",
      "                                 '-b', 'unix:/tmp/gunicorn.sock',\n",
      "                                 '-w', str(model_server_workers),\n",
      "                                 'wsgi:app'])\n",
      "\n",
      "    signal.signal(signal.SIGTERM, lambda a, b: sigterm_handler(nginx.pid, gunicorn.pid))\n",
      "\n",
      "    # If either subprocess exits, so do we.\n",
      "    pids = set([nginx.pid, gunicorn.pid])\n",
      "    while True:\n",
      "        pid, _ = os.wait()\n",
      "        if pid in pids:\n",
      "            break\n",
      "\n",
      "    sigterm_handler(nginx.pid, gunicorn.pid)\n",
      "    print('Inference server exiting')\n",
      "\n",
      "# The main routine just invokes the start function.\n",
      "\n",
      "if __name__ == '__main__':\n",
      "    start_server()\n"
     ]
    }
   ],
   "source": [
    "!cat code/serve"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "# Example neuron-rtd dockerfile.\n",
      "\n",
      "# To build:\n",
      "#    docker build . -f Dockerfile.neuron-rtd -t neuron-rtd\n",
      "\n",
      "# Note: the container must start with CAP_SYS_ADMIN + CAP_IPC_LOCK capabilities in order\n",
      "# to map the memory needed from the Infernetia devices. These capabilities will\n",
      "# be dropped following initialization.\n",
      "\n",
      "# i.e. To start the container with required capabilities:\n",
      "#   docker run --env AWS_NEURON_VISIBLE_DEVICES=\"0\" --cap-add SYS_ADMIN --cap-add IPC_LOCK -v /tmp/neuron_rtd_sock/:/sock neuron-rtd\n",
      "\n",
      "FROM amazonlinux:2\n",
      "\n",
      "MAINTAINER Chaitanya Hazarey <chazarey@amazon.com>\n",
      "\n",
      "RUN echo $'[neuron] \\n\\\n",
      "name=Neuron YUM Repository \\n\\\n",
      "baseurl=https://yum.repos.neuron.amazonaws.com \\n\\\n",
      "enabled=1' > /etc/yum.repos.d/neuron.repo\n",
      "\n",
      "RUN rpm --import https://yum.repos.neuron.amazonaws.com/GPG-PUB-KEY-AMAZON-AWS-NEURON.PUB\n",
      "\n",
      "RUN yum install -y wget \n",
      "RUN yum install -y aws-neuron-tools\n",
      "RUN yum install -y aws-neuron-runtime\n",
      "#RUN yum install -y python\n",
      "RUN yum install -y tar gzip ca-certificates procps net-tools which vim\n",
      "RUN amazon-linux-extras install -y nginx1\n",
      "\n",
      "# Here we get all python packages.\n",
      "# There's substantial overlap between scipy and numpy that we eliminate by\n",
      "# linking them together. Likewise, pip leaves the install caches populated which uses\n",
      "# a significant amount of space. These optimizations save a fair amount of space in the\n",
      "# image, which reduces start up time.\n",
      "#/usr/lib64/python2.7/site-packages/scipy\n",
      "\n",
      "\n",
      "RUN yum install -y python3 libgomp\n",
      "RUN wget https://bootstrap.pypa.io/get-pip.py\n",
      "RUN python3 get-pip.py && \\\n",
      "    python3 -m pip install --upgrade --force-reinstall --no-cache-dir 'numpy<=1.18.2,>=1.13.3' \\ \n",
      "    'neuron-cc[tensorflow]>=1.0.16861.0' torch-neuron\\ \n",
      "    scipy==1.2.1 scikit-learn==0.20.2 pandas flask gevent gunicorn boto3 \\\n",
      "    transformers==2.5.1 --extra-index-url=https://pip.repos.neuron.amazonaws.com    \n",
      "    \n",
      "# Set some environment variables. PYTHONUNBUFFERED keeps Python from buffering our standard\n",
      "# output stream, which means that logs can be delivered to the user quickly. PYTHONDONTWRITEBYTECODE\n",
      "# keeps Python from writing the .pyc files which are unnecessary in this case. We also update\n",
      "# PATH so that the train and serve programs are found when the container is invoked.\n",
      "\n",
      "RUN cp -r /usr/local/lib/python3.7/site-packages/torch/neuron/ /usr/local/lib64/python3.7/site-packages/torch/\n",
      "\n",
      "ENV NEURONCORE_GROUP_SIZES=1\n",
      "ENV PYTHONUNBUFFERED=TRUE\n",
      "ENV PYTHONDONTWRITEBYTECODE=TRUE\n",
      "ENV PATH=\"/opt/program:${PATH}\"\n",
      "\n",
      "# Set up the program in the image\n",
      "COPY code /opt/program\n",
      "ENV PATH=\"/opt/aws/neuron/bin:${PATH}\"\n",
      "\n",
      "CMD neuron-rtd -g unix:/sock/neuron.sock --log-console\n",
      "\n",
      "WORKDIR /opt/program\n",
      "\n"
     ]
    }
   ],
   "source": [
    "!cat Dockerfile.sm.neuron-rtd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "code  Dockerfile.sm.neuron-rtd\tinf1-container.ipynb  inference.py  README.md\n"
     ]
    }
   ],
   "source": [
    "!ls"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Login Succeeded\n",
      "Sending build context to Docker daemon  99.33kB\n",
      "Step 1/21 : FROM amazonlinux:2\n",
      " ---> ba2cc467a2bc\n",
      "Step 2/21 : MAINTAINER Chaitanya Hazarey <chazarey@amazon.com>\n",
      " ---> Using cache\n",
      " ---> 088a31d5941a\n",
      "Step 3/21 : RUN echo $'[neuron] \\nname=Neuron YUM Repository \\nbaseurl=https://yum.repos.neuron.amazonaws.com \\nenabled=1' > /etc/yum.repos.d/neuron.repo\n",
      " ---> Using cache\n",
      " ---> 0769d0beed1b\n",
      "Step 4/21 : RUN rpm --import https://yum.repos.neuron.amazonaws.com/GPG-PUB-KEY-AMAZON-AWS-NEURON.PUB\n",
      " ---> Using cache\n",
      " ---> 5eae2d75e8f3\n",
      "Step 5/21 : RUN yum install -y wget\n",
      " ---> Using cache\n",
      " ---> dfd9fed5d8f0\n",
      "Step 6/21 : RUN yum install -y aws-neuron-tools\n",
      " ---> Using cache\n",
      " ---> 276a5f678a9e\n",
      "Step 7/21 : RUN yum install -y aws-neuron-runtime\n",
      " ---> Using cache\n",
      " ---> 1fd9585a3950\n",
      "Step 8/21 : RUN yum install -y tar gzip ca-certificates procps net-tools which vim\n",
      " ---> Using cache\n",
      " ---> 43815f81991a\n",
      "Step 9/21 : RUN amazon-linux-extras install -y nginx1\n",
      " ---> Using cache\n",
      " ---> d3b2f457d5e9\n",
      "Step 10/21 : RUN yum install -y python3 libgomp\n",
      " ---> Using cache\n",
      " ---> f0560c313050\n",
      "Step 11/21 : RUN wget https://bootstrap.pypa.io/get-pip.py\n",
      " ---> Using cache\n",
      " ---> 56c0a525e4dc\n",
      "Step 12/21 : RUN python3 get-pip.py &&     python3 -m pip install --upgrade --force-reinstall --no-cache-dir 'numpy<=1.18.2,>=1.13.3'     'neuron-cc[tensorflow]>=1.0.16861.0' torch-neuron    scipy==1.2.1 scikit-learn==0.20.2 pandas flask gevent gunicorn boto3     transformers==2.5.1 --extra-index-url=https://pip.repos.neuron.amazonaws.com\n",
      " ---> Using cache\n",
      " ---> 2efe09fa2adb\n",
      "Step 13/21 : RUN cp -r /usr/local/lib/python3.7/site-packages/torch/neuron/ /usr/local/lib64/python3.7/site-packages/torch/\n",
      " ---> Using cache\n",
      " ---> 4d9909e7b776\n",
      "Step 14/21 : ENV NEURONCORE_GROUP_SIZES=1\n",
      " ---> Using cache\n",
      " ---> d7596752d74b\n",
      "Step 15/21 : ENV PYTHONUNBUFFERED=TRUE\n",
      " ---> Using cache\n",
      " ---> 9ba0d9896060\n",
      "Step 16/21 : ENV PYTHONDONTWRITEBYTECODE=TRUE\n",
      " ---> Using cache\n",
      " ---> f375f7256855\n",
      "Step 17/21 : ENV PATH=\"/opt/program:${PATH}\"\n",
      " ---> Using cache\n",
      " ---> 93d2e78fee02\n",
      "Step 18/21 : COPY code /opt/program\n",
      " ---> Using cache\n",
      " ---> 9354257dc14b\n",
      "Step 19/21 : ENV PATH=\"/opt/aws/neuron/bin:${PATH}\"\n",
      " ---> Using cache\n",
      " ---> 2855ceac160c\n",
      "Step 20/21 : CMD neuron-rtd -g unix:/sock/neuron.sock --log-console\n",
      " ---> Using cache\n",
      " ---> b69ceddbc7eb\n",
      "Step 21/21 : WORKDIR /opt/program\n",
      " ---> Using cache\n",
      " ---> db7440f2bf0f\n",
      "Successfully built db7440f2bf0f\n",
      "Successfully tagged chazarey-inf1-serving-cm:latest\n",
      "The push refers to repository [111652037296.dkr.ecr.us-east-1.amazonaws.com/chazarey-inf1-serving-cm]\n",
      "3667b3d55b40: Preparing\n",
      "e86288df2741: Preparing\n",
      "5993ea3053e1: Preparing\n",
      "36db74e707f0: Preparing\n",
      "faa787afd20e: Preparing\n",
      "632481ba49f2: Preparing\n",
      "af86c485bd82: Preparing\n",
      "871961f580a8: Preparing\n",
      "acbd810c8f6d: Preparing\n",
      "c3ad1ea4248c: Preparing\n",
      "20677aae3d40: Preparing\n",
      "3370ed55d11c: Preparing\n",
      "50c3cd231426: Preparing\n",
      "acbd810c8f6d: Waiting\n",
      "c3ad1ea4248c: Waiting\n",
      "20677aae3d40: Waiting\n",
      "3370ed55d11c: Waiting\n",
      "50c3cd231426: Waiting\n",
      "af86c485bd82: Waiting\n",
      "871961f580a8: Waiting\n",
      "632481ba49f2: Waiting\n",
      "3667b3d55b40: Layer already exists\n",
      "faa787afd20e: Layer already exists\n",
      "e86288df2741: Layer already exists\n",
      "36db74e707f0: Layer already exists\n",
      "5993ea3053e1: Layer already exists\n",
      "632481ba49f2: Layer already exists\n",
      "af86c485bd82: Layer already exists\n",
      "871961f580a8: Layer already exists\n",
      "acbd810c8f6d: Layer already exists\n",
      "c3ad1ea4248c: Layer already exists\n",
      "20677aae3d40: Layer already exists\n",
      "50c3cd231426: Layer already exists\n",
      "3370ed55d11c: Layer already exists\n",
      "latest: digest: sha256:ef9c63a398f6f69cc22a7350cce5687e9acd1bdb7b22905e28a20413f1f38d41 size: 3064\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING! Using --password via the CLI is insecure. Use --password-stdin.\n",
      "WARNING! Your password will be stored unencrypted in /home/ec2-user/.docker/config.json.\n",
      "Configure a credential helper to remove this warning. See\n",
      "https://docs.docker.com/engine/reference/commandline/login/#credentials-store\n",
      "\n"
     ]
    }
   ],
   "source": [
    "%%sh \n",
    "\n",
    "# The name of our algorithm\n",
    "algorithm_name=chazarey-inf1-serving-cm\n",
    "account=$(aws sts get-caller-identity --query Account --output text)\n",
    "\n",
    "# Get the region defined in the current configuration (default to us-west-2 if none defined)\n",
    "region=$(aws configure get region)\n",
    "fullname=\"${account}.dkr.ecr.${region}.amazonaws.com/${algorithm_name}:latest\"\n",
    "\n",
    "# If the repository doesn't exist in ECR, create it.\n",
    "aws ecr describe-repositories --repository-names \"${algorithm_name}\" > /dev/null 2>&1\n",
    "\n",
    "if [ $? -ne 0 ]\n",
    "then\n",
    "    aws ecr create-repository --repository-name \"${algorithm_name}\" > /dev/null\n",
    "fi\n",
    "\n",
    "# Get the login command from ECR and execute it directly\n",
    "$(aws ecr get-login --region ${region} --no-include-email)\n",
    "\n",
    "# Build the docker image locally with the image name and then push it to ECR\n",
    "\n",
    "docker build -t ${algorithm_name} -f Dockerfile.sm.neuron-rtd .\n",
    "docker tag ${algorithm_name} ${fullname}\n",
    "docker push ${fullname}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "#!pip install 'sagemaker[local]' --upgrade"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Parameter image will be renamed to image_uri in SageMaker Python SDK v2.\n",
      "Your model is not compiled. Please compile your model before using Inferentia.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "-------------!CPU times: user 18.4 s, sys: 2.67 s, total: 21.1 s\n",
      "Wall time: 6min 52s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "\n",
    "import sagemaker\n",
    "from sagemaker.pytorch import PyTorchModel\n",
    "from sagemaker.predictor import csv_serializer\n",
    "\n",
    "role = sagemaker.session.get_execution_role()\n",
    "\n",
    "model_data='s3://inf1-compiled-model/model.tar.gz'\n",
    "\n",
    "pytorch_model = PyTorchModel(model_data=model_data, \n",
    "                             role=role,\n",
    "                             entry_point='inference.py',\n",
    "                             image='111652037296.dkr.ecr.us-east-1.amazonaws.com/chazarey-inf1-serving-cm:latest',\n",
    "                             framework_version='1.5.0',\n",
    "                             enable_cloudwatch_metrics=True)\n",
    "\n",
    "predictor = pytorch_model.deploy(instance_type='ml.inf1.xlarge', initial_instance_count=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: transformers in /home/ec2-user/anaconda3/envs/pytorch_p36/lib/python3.6/site-packages (3.1.0)\n",
      "Requirement already satisfied: dataclasses; python_version < \"3.7\" in /home/ec2-user/anaconda3/envs/pytorch_p36/lib/python3.6/site-packages (from transformers) (0.7)\n",
      "Requirement already satisfied: filelock in /home/ec2-user/anaconda3/envs/pytorch_p36/lib/python3.6/site-packages (from transformers) (3.0.12)\n",
      "Requirement already satisfied: regex!=2019.12.17 in /home/ec2-user/anaconda3/envs/pytorch_p36/lib/python3.6/site-packages (from transformers) (2020.6.8)\n",
      "Requirement already satisfied: packaging in /home/ec2-user/anaconda3/envs/pytorch_p36/lib/python3.6/site-packages (from transformers) (20.4)\n",
      "Requirement already satisfied: tqdm>=4.27 in /home/ec2-user/anaconda3/envs/pytorch_p36/lib/python3.6/site-packages (from transformers) (4.47.0)\n",
      "Requirement already satisfied: sacremoses in /home/ec2-user/anaconda3/envs/pytorch_p36/lib/python3.6/site-packages (from transformers) (0.0.43)\n",
      "Requirement already satisfied: tokenizers==0.8.1.rc2 in /home/ec2-user/anaconda3/envs/pytorch_p36/lib/python3.6/site-packages (from transformers) (0.8.1rc2)\n",
      "Requirement already satisfied: sentencepiece!=0.1.92 in /home/ec2-user/anaconda3/envs/pytorch_p36/lib/python3.6/site-packages (from transformers) (0.1.91)\n",
      "Requirement already satisfied: requests in /home/ec2-user/anaconda3/envs/pytorch_p36/lib/python3.6/site-packages (from transformers) (2.24.0)\n",
      "Requirement already satisfied: numpy in /home/ec2-user/anaconda3/envs/pytorch_p36/lib/python3.6/site-packages (from transformers) (1.18.5)\n",
      "Requirement already satisfied: six in /home/ec2-user/anaconda3/envs/pytorch_p36/lib/python3.6/site-packages (from packaging->transformers) (1.15.0)\n",
      "Requirement already satisfied: pyparsing>=2.0.2 in /home/ec2-user/anaconda3/envs/pytorch_p36/lib/python3.6/site-packages (from packaging->transformers) (2.4.7)\n",
      "Requirement already satisfied: joblib in /home/ec2-user/anaconda3/envs/pytorch_p36/lib/python3.6/site-packages (from sacremoses->transformers) (0.16.0)\n",
      "Requirement already satisfied: click in /home/ec2-user/anaconda3/envs/pytorch_p36/lib/python3.6/site-packages (from sacremoses->transformers) (7.1.2)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in /home/ec2-user/anaconda3/envs/pytorch_p36/lib/python3.6/site-packages (from requests->transformers) (2020.6.20)\n",
      "Requirement already satisfied: chardet<4,>=3.0.2 in /home/ec2-user/anaconda3/envs/pytorch_p36/lib/python3.6/site-packages (from requests->transformers) (3.0.4)\n",
      "Requirement already satisfied: idna<3,>=2.5 in /home/ec2-user/anaconda3/envs/pytorch_p36/lib/python3.6/site-packages (from requests->transformers) (2.10)\n",
      "Requirement already satisfied: urllib3!=1.25.0,!=1.25.1,<1.26,>=1.21.1 in /home/ec2-user/anaconda3/envs/pytorch_p36/lib/python3.6/site-packages (from requests->transformers) (1.25.9)\n",
      "\u001b[33mWARNING: You are using pip version 20.1.1; however, version 20.2.2 is available.\n",
      "You should consider upgrading via the '/home/ec2-user/anaconda3/envs/pytorch_p36/bin/python -m pip install --upgrade pip' command.\u001b[0m\n"
     ]
    }
   ],
   "source": [
    "!pip install transformers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/ec2-user/anaconda3/envs/pytorch_p36/lib/python3.6/site-packages/transformers/tokenization_utils_base.py:1770: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n",
      "  FutureWarning,\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import transformers\n",
    "from transformers import BertTokenizer\n",
    "from transformers import BertModel\n",
    "import math\n",
    "import numpy as np\n",
    "import io\n",
    "from sagemaker.predictor import csv_serializer, json_deserializer\n",
    "from transformers import AutoTokenizer, AutoModelForSequenceClassification\n",
    "import pickle \n",
    "\n",
    "def numpy_bytes_serializer(data):\n",
    "    f = io.BytesIO()\n",
    "    np.save(f, data)\n",
    "    f.seek(0)\n",
    "    return f.read()\n",
    "\n",
    "# vnd indicates vendor-specific MIME types, which means they are MIME types that were \n",
    "# introduced by corporate bodies rather than e.g. an Internet consortium.\n",
    "\n",
    "\n",
    "#-predictor.serializer = numpy_bytes_serializer\n",
    "#-predictor.deserializer = csv_serializer\n",
    "\n",
    "predictor.content_type = 'application/binary'\n",
    "predictor.serializer = None\n",
    "predictor.deserializer = None\n",
    "\n",
    "sentence1=\"If you set your goals ridiculously high and it's a failure, you will fail above everyone else's success.\"\n",
    "sentence2=\"The greatest glory in living lies not in never falling, but in rising every time we fall.\"\n",
    "sentence3=\"If you set your goals ridiculously high and it's a failure, you will fail above everyone else's success. If you set your goals ridiculously high and it's a failure, you will fail above everyone else's success. If you set your goals ridiculously high and it's a failure, you will fail above everyone else's success.\"\n",
    "\n",
    "#tokenizer = BertTokenizer.from_pretrained('bert-base-uncased')\n",
    "tokenizer = AutoTokenizer.from_pretrained('bert-base-uncased')\n",
    "\n",
    "cos = torch.nn.CosineSimilarity()\n",
    "\n",
    "encoded_sentence = tokenizer.encode_plus(sentence1, sentence3, max_length=128, pad_to_max_length=True, return_tensors=\"pt\", truncation=True)\n",
    "encoded_sentence_tuple = encoded_sentence['input_ids'], encoded_sentence['attention_mask'], encoded_sentence['token_type_ids'] \n",
    "\n",
    "try:\n",
    "    pickled_bytes = pickle.dumps(encoded_sentence_tuple)\n",
    "    raw_bytes = predictor.predict(pickled_bytes)\n",
    "except:\n",
    "    pass\n",
    "    \n",
    "    \n",
    "#print(pickle.loads(raw_bytes))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "#type(encoded_sentence_tuple)#\n",
    "#import pickle \n",
    "#msg = pickle.dumps(encoded_sentence_tuple)\n",
    "#recd = pickle.loads(msg)\n",
    "#type(recd)\n",
    "#recd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "13.970366695632904\n",
      "16.322904834111583\n",
      "30.797703771720812\n",
      "10.04106705374436\n",
      "91.31834721263706\n",
      "27.373670006557752\n",
      "96.60643695227958\n",
      "87.13354474243069\n",
      "108.36988154583487\n",
      "23.193931968233258\n"
     ]
    }
   ],
   "source": [
    "import random\n",
    "\n",
    "s_nouns = [\"A dude\", \"My mom\", \"The king\", \"Some guy\", \"A cat with rabies\", \"A sloth\", \"Your homie\", \"This cool guy my gardener met yesterday\", \"Superman\"]\n",
    "p_nouns = [\"These dudes\", \"Both of my moms\", \"All the kings of the world\", \"Some guys\", \"All of a cattery's cats\", \"The multitude of sloths living under your bed\", \"Your homies\", \"Like, these, like, all these people\", \"Supermen\"]\n",
    "s_verbs = [\"eats\", \"kicks\", \"gives\", \"treats\", \"meets with\", \"creates\", \"hacks\", \"configures\", \"spies on\", \"retards\", \"meows on\", \"flees from\", \"tries to automate\", \"explodes\"]\n",
    "p_verbs = [\"eat\", \"kick\", \"give\", \"treat\", \"meet with\", \"create\", \"hack\", \"configure\", \"spy on\", \"retard\", \"meow on\", \"flee from\", \"try to automate\", \"explode\"]\n",
    "infinitives = [\"to make a pie.\", \"for no apparent reason.\", \"because the sky is green.\", \"for a disease.\", \"to be able to make toast explode.\", \"to know more about archeology.\"]\n",
    "\n",
    "for i in range(0,10):    \n",
    "    sentence_random = (random.choice(s_nouns) + ' ' + random.choice(s_verbs) + ' ' + random.choice(s_nouns).lower() or random.choice(p_nouns).lower() + ' ' + random.choice(infinitives))\n",
    "    encoded_sentence = tokenizer.encode_plus(sentence1, sentence_random, max_length=128, pad_to_max_length=True, return_tensors=\"pt\", truncation=True)\n",
    "    encoded_sentence_tuple = encoded_sentence['input_ids'], encoded_sentence['attention_mask'], encoded_sentence['token_type_ids'] \n",
    "    pickled_bytes = pickle.dumps(encoded_sentence_tuple)\n",
    "    \n",
    "    try:\n",
    "        raw_bytes = predictor.predict(pickled_bytes)\n",
    "        embeddings_returned = pickle.loads(raw_bytes)\n",
    "        s1 = embeddings_returned[1]\n",
    "        #print(pickle.loads(raw_bytes))\n",
    "    except:\n",
    "        pass\n",
    "\n",
    "    sentence_random = (random.choice(s_nouns) + ' ' + random.choice(s_verbs) + ' ' + random.choice(s_nouns).lower() or random.choice(p_nouns).lower() + ' ' + random.choice(infinitives))\n",
    "    encoded_sentence = tokenizer.encode_plus(sentence1, sentence_random, max_length=128, pad_to_max_length=True, return_tensors=\"pt\", truncation=True)\n",
    "    encoded_sentence_tuple = encoded_sentence['input_ids'], encoded_sentence['attention_mask'], encoded_sentence['token_type_ids'] \n",
    "    pickled_bytes = pickle.dumps(encoded_sentence_tuple)\n",
    "    \n",
    "    try:\n",
    "        raw_bytes = predictor.predict(pickled_bytes)\n",
    "        embeddings_returned = pickle.loads(raw_bytes)\n",
    "        s2 = embeddings_returned[1]\n",
    "        #print(pickle.loads(raw_bytes))\n",
    "    except:\n",
    "        pass    \n",
    "    \n",
    "    cos = torch.nn.CosineSimilarity()    \n",
    "    s2 = embeddings_returned[1]\n",
    "    cos_sim = cos(s1,s2)\n",
    "    cosine_measure = cos_sim[0].item()\n",
    "    angle_in_radians = math.acos(cosine_measure)\n",
    "    print(math.degrees(angle_in_radians))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np \n",
    "import datetime\n",
    "import math\n",
    "import time\n",
    "import boto3   \n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "\n",
    "endpoint_name=predictor.endpoint\n",
    "total_runs=10000"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Running 10000 inferences for chazarey-inf1-serving-cm-2020-09-08-20-04-39-510:\n"
     ]
    }
   ],
   "source": [
    "print('Running {} inferences for {}:'.format(total_runs, endpoint_name))\n",
    "\n",
    "client_times = []\n",
    "errors_list = []\n",
    "cw_start = datetime.datetime.utcnow()\n",
    "\n",
    "errors = 0\n",
    "\n",
    "for i in range(total_runs):    \n",
    "    \n",
    "    client_start = time.time()\n",
    "    \n",
    "    sentence_random = (random.choice(s_nouns) + ' ' + random.choice(s_verbs) + ' ' + random.choice(s_nouns).lower() or random.choice(p_nouns).lower() + ' ' + random.choice(infinitives))\n",
    "    encoded_sentence = tokenizer.encode_plus(sentence1, sentence_random, max_length=128, pad_to_max_length=True, return_tensors=\"pt\", truncation=True)\n",
    "    encoded_sentence_tuple = encoded_sentence['input_ids'], encoded_sentence['attention_mask'], encoded_sentence['token_type_ids'] \n",
    "    \n",
    "    pickled_bytes = pickle.dumps(encoded_sentence_tuple)\n",
    "\n",
    "    try:\n",
    "        raw_bytes = predictor.predict(pickled_bytes)\n",
    "        errors_list.append(20)\n",
    "    except:\n",
    "        errors += 1\n",
    "        errors_list.append(30)\n",
    "        pass\n",
    "    \n",
    "    client_end = time.time()\n",
    "    client_times.append((client_end - client_start)*1000)\n",
    "    \n",
    "print('\\nErrors - {:.4f} out of {:.4f} total runs | {:.4f}% \\n'.format(errors, total_runs, (errors/total_runs)*100))\n",
    "errors = 0\n",
    "    \n",
    "    \n",
    "cw_end = datetime.datetime.utcnow()    \n",
    "    \n",
    "print('Client end-to-end latency percentiles:')\n",
    "client_avg = np.mean(client_times)\n",
    "client_p50 = np.percentile(client_times, 50)\n",
    "client_p90 = np.percentile(client_times, 90)\n",
    "client_p95 = np.percentile(client_times, 95)\n",
    "client_p100 = np.percentile(client_times, 100)\n",
    "print('Avg | P50 | P90 | P95 | P100')\n",
    "print('{:.4f} | {:.4f} | {:.4f} | {:.4f}\\n'.format(client_avg, client_p50, client_p90, client_p95, client_p100))\n",
    "\n",
    "print('Getting Cloudwatch:')\n",
    "cloudwatch = boto3.client('cloudwatch')\n",
    "statistics=['SampleCount', 'Average', 'Minimum', 'Maximum']\n",
    "extended=['p50', 'p90', 'p95', 'p100']\n",
    "\n",
    "# Give 5 minute buffer to end\n",
    "cw_end += datetime.timedelta(minutes=5)\n",
    "\n",
    "# Period must be 1, 5, 10, 30, or multiple of 60\n",
    "# Calculate closest multiple of 60 to the total elapsed time\n",
    "factor = math.ceil((cw_end - cw_start).total_seconds() / 60)\n",
    "period = factor * 60\n",
    "print('Time elapsed: {} seconds'.format((cw_end - cw_start).total_seconds()))\n",
    "print('Using period of {} seconds\\n'.format(period))\n",
    "\n",
    "cloudwatch_ready = False\n",
    "# Keep polling CloudWatch metrics until datapoints are available\n",
    "while not cloudwatch_ready:\n",
    "  time.sleep(30)\n",
    "  print('Waiting 30 seconds ...')\n",
    "  # Must use default units of microseconds\n",
    "  model_latency_metrics = cloudwatch.get_metric_statistics(MetricName='ModelLatency',\n",
    "                                             Dimensions=[{'Name': 'EndpointName',\n",
    "                                                          'Value': endpoint_name},\n",
    "                                                         {'Name': 'VariantName',\n",
    "                                                          'Value': \"AllTraffic\"}],\n",
    "                                             Namespace=\"AWS/SageMaker\",\n",
    "                                             StartTime=cw_start,\n",
    "                                             EndTime=cw_end,\n",
    "                                             Period=period,\n",
    "                                             Statistics=statistics,\n",
    "                                             ExtendedStatistics=extended\n",
    "                                             )\n",
    "  # Should be 1000\n",
    "  if len(model_latency_metrics['Datapoints']) > 0:\n",
    "    print('{} latency datapoints ready'.format(model_latency_metrics['Datapoints'][0]['SampleCount']))\n",
    "    side_avg = model_latency_metrics['Datapoints'][0]['Average'] / total_runs\n",
    "    side_p50 = model_latency_metrics['Datapoints'][0]['ExtendedStatistics']['p50'] / total_runs\n",
    "    side_p90 = model_latency_metrics['Datapoints'][0]['ExtendedStatistics']['p90'] / total_runs\n",
    "    side_p95 = model_latency_metrics['Datapoints'][0]['ExtendedStatistics']['p95'] / total_runs\n",
    "    side_p100 = model_latency_metrics['Datapoints'][0]['ExtendedStatistics']['p100'] / total_runs\n",
    "    print('Avg | P50 | P90 | P95 | P100')\n",
    "    print('{:.4f} | {:.4f} | {:.4f} | {:.4f}\\n'.format(side_avg, side_p50, side_p90, side_p95, side_p100))\n",
    "\n",
    "    cloudwatch_ready = True\n",
    "    \n",
    "    #embeddings_returned = pickle.loads(raw_bytes)\n",
    "    #s1 = embeddings_returned[1]\n",
    "    #print(pickle.loads(raw_bytes))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig = plt.figure(figsize=(80, 60))\n",
    "ax = fig.add_subplot(111)\n",
    "ax.plot(client_times)\n",
    "ax.plot(errors_list)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#client_times"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "c = '''client_times = []\n",
    "cw_start = datetime.datetime.utcnow()\n",
    "\n",
    "client_start = time.time()\n",
    "\n",
    "time.sleep(1)\n",
    "\n",
    "client_end = time.time()\n",
    "\n",
    "client_times.append((client_end - client_start))\n",
    "\n",
    "cw_end = datetime.datetime.utcnow()    \n",
    "    \n",
    "print('Client end-to-end latency percentiles:')\n",
    "client_avg = np.mean(client_times)\n",
    "client_p50 = np.percentile(client_times, 50)\n",
    "client_p90 = np.percentile(client_times, 90)\n",
    "client_p95 = np.percentile(client_times, 95)\n",
    "client_p100 = np.percentile(client_times, 100)\n",
    "print('Avg | P50 | P90 | P95 | P100')\n",
    "print('{:.4f} | {:.4f} | {:.4f} | {:.4f}\\n'.format(client_avg, client_p50, client_p90, client_p95, client_p100))\n",
    "'''"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "conda_pytorch_p36",
   "language": "python",
   "name": "conda_pytorch_p36"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
